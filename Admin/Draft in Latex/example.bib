@article{Rodriguez2018,
abstract = {We study connections between the alternating direction method of multipliers (ADMM), the classical method of multipliers (MM), and progressive hedging (PH). The connections are used to derive benchmark metrics and strategies to monitor and accelerate convergence and to help explain why ADMM and PH are capable of solving complex nonconvex NLPs. Specifically, we observe that ADMM is an inexact version of MM and approaches its performance when multiple coordination steps are performed. In addition, we use the observation that PH is a specialization of ADMM and borrow Lyapunov function and primal-dual feasibility metrics used in ADMM to explain why PH is capable of solving nonconvex NLPs. This analysis also highlights that specialized PH schemes can be derived to tackle a wider range of stochastic programs and even other problem classes. Our exposition is tutorial in nature and seeks to to motivate algorithmic improvements and new decomposition strategies},
author = {Rodriguez, Jose S. and Nicholson, Bethany and Laird, Carl and Zavala, Victor M.},
doi = {10.1016/j.compchemeng.2018.08.036},
file = {:C$\backslash$:/Users/sandeepp/Downloads/1-s2.0-S0098135418305210-main.pdf:pdf},
issn = {00981354},
journal = {Computers and Chemical Engineering},
keywords = {ADMM,Augmented Lagrangian,Coordination,Decomposition,Large-scale,NLP},
pages = {315--325},
publisher = {Elsevier Ltd},
title = {{Benchmarking ADMM in nonconvex NLPs}},
url = {https://doi.org/10.1016/j.compchemeng.2018.08.036},
volume = {119},
year = {2018}
}
@phdthesis{Prakash2020,
author = {Prakash, Sandeep},
file = {:C$\backslash$:/Users/sandeepp/OneDrive - NTNU/NTNU{\_}MSc{\_}files/Thesis{\_}Files/Admin Files/Thesis/Sandeep{\_}Thesis{\_}FinalUpload.pdf:pdf},
school = {Norwegian University of Science and Technology, Norway},
title = {{Distributed optimization using ADMM for Optimal Design of Thermal Energy Storage systems}},
type = {Master's Thesis},
year = {2020}
}
@incollection{THOMBRE20201459,
abstract = {A key factor for energy-efficient industrial clusters is the recovery of waste heat. To this end, thermal energy storage (TES) is an appealing technology that facilitates dynamic heat integration between supplier and consumer plants. A long-term strategy for energy savings must involve adequate consideration for the optimal design of the TES. From an industrial perspective, finding the capacity of the TES unit is often based on heuristic rules which may lead to suboptimal design. This approach does not account for the short-term variability in operation of the TES system. Scenario-based stochastic programming approaches, where the operational uncertainty is described in form of discrete scenarios, can be used to find the best design for the TES system. We present two problem formulations for finding the optimal capacity of the TES unit. The first is a single-level formulation where the design and operating constraints are combined for all scenarios, with the objective of minimizing the combined cost of design and operation. The second is a bilevel formulation where the design decisions are taken on the upper level to minimize overall system cost, whereas the lower level problems (one per scenario) represent the optimal operation for the chosen design variables, each minimizing the operating cost for their respective scenarios. We compare the results of the two approaches with an illustrative case study of an industrial cluster with one supplier plant and one consumer plant exchanging heat via a TES unit.},
author = {Thombre, Mandar and Prakash, Sandeep and Knudsen, Brage Rugstad and J{\"{a}}schke, Johannes},
booktitle = {30th European Symposium on Computer Aided Process Engineering},
doi = {https://doi.org/10.1016/B978-0-12-823377-1.50244-5},
editor = {Pierucci, Sauro and Manenti, Flavio and Bozzano, Giulia Luisa and Manca, Davide},
issn = {1570-7946},
keywords = { bilevel programming, industrial cluster,thermal energy storage},
pages = {1459--1464},
publisher = {Elsevier},
series = {Computer Aided Chemical Engineering},
title = {{Optimizing the Capacity of Thermal Energy Storage in Industrial Clusters}},
url = {http://www.sciencedirect.com/science/article/pii/B9780128233771502445},
volume = {48},
year = {2020}
}
@article{Boyd2010,
abstract = {Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas-Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for â„“1 problems, proximal methods, and thers. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations. {\textcopyright} 2011 S. Boyd, N. Parikh, E. Chu, B. Peleato and J. Eckstein.},
author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
doi = {10.1561/2200000016},
file = {:C$\backslash$:/Users/sandeepp/Downloads/admm{\_}distr{\_}stats.pdf:pdf},
issn = {19358237},
journal = {Foundations and Trends in Machine Learning},
number = {1},
pages = {1--122},
title = {{Distributed optimization and statistical learning via the alternating direction method of multipliers}},
volume = {3},
year = {2010}
}
